{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Week 10-11: Document Classification\n",
    "\n",
    "*CUNY SPS DATA 620*  \n",
    "\n",
    "*April 22, 2022*\n",
    "\n",
    "*Bonnie Cooper, George Deschamps, Rob Hodde*\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  UCI Machine Learning Repository: Spambase Data Set\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We will use the Kaggle competition, **Natural Language Processing with Disaster Tweets.**  (https://www.kaggle.com/competitions/nlp-getting-started/overview).  \n",
    "\n",
    "The challenge is to build a machine learning model to distinguish between Tweets that are about real disasters and those that are not. \n",
    "\n",
    "<br>\n",
    "\n",
    "We start by importing packages for data collection and cleansing, processing and finally prediction modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import os\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "\n",
    "import cleantext  \n",
    "from emoji import demojize\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a helper function to scrub the Tweets to be more legible to a computer. This includes removing numbers, punctuation, and words that carry little information. We also translate emoticons into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes text to lower case\n",
    "# Removes:\n",
    "#    numbers and punctuation \n",
    "#    stopwords\n",
    "#    extra spaces\n",
    "# Translates emoji's into phrases \n",
    "def clean_text(x):\n",
    "    x = demojize(x, language='alias') \n",
    "    x = re.sub(r\"[:]+\\ *\", \" \", x) #removes emoji colons and separates them with a space\n",
    "    return cleantext.clean(x, extra_spaces=True, lowercase=True, numbers=True, punct=True, stopwords=True,\n",
    "                     reg=r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", reg_replace=' ')\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatize function converts variations of words into their root form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Lemmatize text (convert various forms to root words) \n",
    "def lemmatize_word(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine functions together to clean, tokenize (create a unique ID for), and lemmatize all the words in the Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rationalize the text: clean, tokenize and lemmatize \n",
    "def rationalize_text(txt):\n",
    "    return txt.apply(lambda x: clean_text(x)).apply(word_tokenize).apply(lambda x: lemmatize_word(x)).apply(lambda x: ''.join(i+' ' for i in x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the Tweets, separated into a Training set, and a Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0                 Just happened a terrible car crash\n",
       "1   2  Heard about #earthquake is different cities, s...\n",
       "2   3  there is a forest fire at spot pond, geese are...\n",
       "3   9           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import train and test data into dataframes\n",
    "os.chdir('C:\\\\Users\\\\TRADE\\\\Documents\\\\GitHub\\\\DATA620-Week11\\\\')\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "train_df = df[['text','target']].copy()\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "test_df = df[['id','text']].copy()\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the Tweets to be more legible to a computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0                       happened terrible car crash \n",
       "1   2  heard earthquake different city stay safe ever...\n",
       "2   3  forest fire spot pond goose fleeing across str...\n",
       "3   9              apocalypse lighting spokane wildfire \n",
       "4  11                typhoon soudelor kill china taiwan "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"] = rationalize_text(train_df[\"text\"])\n",
    "test_df[\"text\"] = rationalize_text(test_df[\"text\"])\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 80% of the Training Tweets to build a prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5701    zakbagans pet r like part family love animal l...\n",
       "2591    black eye space battle occurred star involving...\n",
       "5835    china stock market crash gem rubble chinaûªs ...\n",
       "2085                             thats val dead im suing \n",
       "6251    new photo oak snowstorm http tcojhscgdag south...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split training set so that a model can be built and evaluated\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['target'], test_size=0.2)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help separate actual emergency Tweets from non-emergency ones, we use two tools: Support Vector Machine (SVM) and Term Frequency Inverse Document Frequency (TFIDF).\n",
    "\n",
    "A SVM attempts to draw a line along a theoretical plane, that separates one class of documents from another (in our case, Emergency / Non-Emergency).\n",
    "\n",
    "In order for the documents to be placed in a theoretical plane, they need coordinates. TFIDF supplies (imbeds) these coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 800)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Term Frequency Inverse Document Frequency, vectorize the text so that it can be mapped in high dimensional space\n",
    "#Then use Principal Component Analysis Singular Value Decomposition to eliminate low-predictive-value words\n",
    "train_text = TruncatedSVD(n_components=750).fit_transform(TfidfVectorizer().fit_transform(X_train))\n",
    "train_text.shape\n",
    "\n",
    "# For the Support Vector function above we tried a range of values to settle on the 750 word count:\n",
    "# Words\t    Precision\tRecall\n",
    "# 200\t\t79.6%\t\t75.6%\n",
    "# 500\t\t80.3%\t\t77.1%\n",
    "# 700\t\t81.3%\t\t78.5%\n",
    "# 750\t\t81.7%\t\t78.8%\n",
    "# 800\t\t81.4%\t\t78.7%\n",
    "# 1000\t    81.6%\t\t78.4%\n",
    "# 3000\t    80.0%\t\t77.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a prediction model. We use cross-validation to make the results more robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.814 (0.016)\n",
      "Recall: 0.787 (-0.017)\n"
     ]
    }
   ],
   "source": [
    "#adapted from https://towardsdatascience.com/write-a-document-classifier-in-less-than-30-minutes-2d96a8a8820c\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "model = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo')\n",
    "metrics = cross_validate(model, train_text, y_train.values, scoring=['precision_macro', 'recall_macro'], cv=cv, n_jobs=-1)\n",
    "\n",
    "print('Precision: %.3f (%.3f)' % (mean(metrics[\"test_precision_macro\"]), std(metrics[\"test_precision_macro\"])))\n",
    "print('Recall: %.3f (%.3f)' % (mean(metrics[\"test_recall_macro\"]), -std(metrics[\"test_recall_macro\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs reasonably well, identifying actual emergencies correctly four out of five times, and discovering four out of five actual emergencies.  However, this leaves a lot of room for improvement, as this level of accuracy is not good enough to deploy in an emergency response organization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
